[core]
# The home folder for airflow, default is ~/airflow
airflow_home = /home/yousef/workspace/storage/UZ2CcrTd13NrEAm81F1qLWHyAiD2/projects/rul-prediction-system/airflow

# The folder where your airflow pipelines live
dags_folder = /home/yousef/workspace/storage/UZ2CcrTd13NrEAm81F1qLWHyAiD2/projects/rul-prediction-system/airflow/dags

# The folder where airflow should store its log files
base_log_folder = /home/yousef/workspace/storage/UZ2CcrTd13NrEAm81F1qLWHyAiD2/projects/rul-prediction-system/airflow/logs

# Airflow can store logs remotely in AWS S3, Google Cloud Storage or Elastic Search
remote_logging = False

# Logging level
logging_level = INFO

# Logging class to use for task logs
task_log_reader = task

# The executor class that airflow should use
# LocalExecutor: Runs tasks locally in separate processes
# SequentialExecutor: Runs tasks sequentially (development only)
# CeleryExecutor: Distributed task execution
executor = LocalExecutor

# The SqlAlchemy connection string to the metadata database
# SQLite (development)
# sql_alchemy_conn = sqlite:////home/yousef/workspace/storage/UZ2CcrTd13NrEAm81F1qLWHyAiD2/projects/rul-prediction-system/airflow/airflow.db

# PostgreSQL (production)
sql_alchemy_conn = postgresql+psycopg2://airflow:airflow@localhost:5432/airflow

# The amount of parallelism as a setting to the executor
parallelism = 32

# The number of task instances allowed to run concurrently by the scheduler per DAG
dag_concurrency = 16

# The maximum number of active DAG runs per DAG
max_active_runs_per_dag = 16

# Whether to load the default connections that ship with Airflow
load_default_connections = True

# Whether to load the examples that ship with Airflow
load_examples = False

# Whether to enable pickling for xcom
enable_xcom_pickling = True

# Maximum number of threads to use to check the health of the tasks
max_threads = 4

# Time zone
default_timezone = UTC

# The default owner assigned to each new operator
default_owner = airflow

# Default task retries
default_task_retries = 2

# Default retry delay
default_task_retry_delay = 300

# Whether to catch up by default or not
catchup_by_default = False

# Number of seconds after which a task is failed if it hasn't reported heartbeat
task_heartbeat_sec = 5

# How long before timing out a python file import
dagbag_import_timeout = 30

# How often (in seconds) to scan the DAGs directory for new files
dag_dir_list_interval = 300

# Whether to hide paused DAGs in the UI
hide_paused_dags_by_default = False

# Maximum number of rendered task instance fields (templated fields) per task
max_num_rendered_ti_fields_per_task = 30

[database]
# SQL alchemy pool settings
sql_alchemy_pool_enabled = True
sql_alchemy_pool_size = 5
sql_alchemy_max_overflow = 10
sql_alchemy_pool_recycle = 3600
sql_alchemy_pool_pre_ping = True

[logging]
# Logging level
logging_level = INFO

# Logging format
log_format = [%%(asctime)s] {%%(filename)s:%%(lineno)d} %%(levelname)s - %%(message)s
simple_log_format = %%(asctime)s %%(levelname)s - %%(message)s

# Log filename template
log_filename_template = {{ ti.dag_id }}/{{ ti.task_id }}/{{ ts }}/{{ try_number }}.log

# Task log prefix template
log_processor_filename_template = {{ filename }}.log

# DAG processor manager log location
dag_processor_manager_log_location = /home/yousef/workspace/storage/UZ2CcrTd13NrEAm81F1qLWHyAiD2/projects/rul-prediction-system/airflow/logs/dag_processor_manager/dag_processor_manager.log

# Colored logs
colored_console_log = True
colored_log_format = [%%(blue)s%%(asctime)s%%(reset)s] {%%(blue)s%%(filename)s:%%(reset)s%%(lineno)d} %%(log_color)s%%(levelname)s%%(reset)s - %%(log_color)s%%(message)s%%(reset)s
colored_formatter_class = airflow.utils.log.colored_log.CustomTTYColoredFormatter

[metrics]
# Enable sending metrics to StatsD
statsd_on = False
statsd_host = localhost
statsd_port = 8125
statsd_prefix = airflow

[scheduler]
# How often to run a scheduling loop (in seconds)
scheduler_heartbeat_sec = 5

# Number of seconds after which a DAG file is parsed again
min_file_process_interval = 30

# Number of seconds after which a stale DAG run is marked as failed
dag_file_processor_timeout = 50

# Number of parsing processes
max_threads = 2

# Whether to print statistics to the log
print_stats_interval = 30

# Maximum number of Tis to be scheduled per loop
max_tis_per_query = 512

# Should the scheduler schedule new tasks using Task Instances
scheduler_zombie_task_threshold = 300

# How long tasks should be allowed to run before timing out
scheduler_health_check_threshold = 30

# Turn off scheduler catchup by default
catchup_by_default = False

# Number of DagRuns to look back for rescheduling
max_dagruns_to_create_per_loop = 10

[webserver]
# The base url of your website
base_url = http://localhost:8080

# Default timezone to display all dates in the UI
default_ui_timezone = UTC

# The ip specified when starting the web server
web_server_host = 0.0.0.0

# The port on which to run the web server
web_server_port = 8080

# Number of workers to run the webserver on
workers = 4

# The worker class for gunicorn
worker_class = sync

# Secret key for Flask session
secret_key = your_secret_key_here

# Number of seconds the gunicorn webserver waits before timing out on a worker
web_server_worker_timeout = 120

# Whether to reload the web server when the source code changes
reload_on_plugin_change = True

# Expose the configuration file in the web server
expose_config = False

# Expose hostname in the web server
expose_hostname = True

# Expose stacktrace in the web server
expose_stacktrace = True

# Default DAG view
default_dag_run_display_number = 25

# Number of rendered task instance fields per task
max_num_rendered_ti_fields_per_task = 30

# Default number of items to show in the UI
page_size = 100

# Enable RBAC (Role-Based Access Control)
rbac = True

# Default wrap for task log in the UI
default_wrap = False

[email]
# Email backend to use
email_backend = airflow.utils.email.send_email_smtp

[smtp]
# SMTP server configuration
smtp_host = localhost
smtp_starttls = True
smtp_ssl = False
smtp_port = 25
smtp_mail_from = airflow@example.com

[celery]
# Celery configuration (if using CeleryExecutor)
celery_app_name = airflow.executors.celery_executor

# The Celery broker URL
broker_url = redis://localhost:6379/0

# The Celery result backend
result_backend = db+postgresql://airflow:airflow@localhost:5432/airflow

# Number of Celery workers
worker_concurrency = 16

# Time in seconds before a task is declared as failed
celery_task_track_started = True

# Task acknowledgement
task_acks_late = True

# Celery flower
flower_host = 0.0.0.0
flower_port = 5555

[operators]
# Default owner for operators
default_owner = airflow

# Default queue for operators
default_queue = default

# Whether to respect the Task Instance XCom limit
allow_illegal_arguments = False

[api]
# Enable the API
auth_backend = airflow.api.auth.backend.basic_auth

# Maximum page size for API
maximum_page_limit = 100

[lineage]
# Enable data lineage tracking
backend =

[atlas]
# Atlas configuration
sasl_enabled = False
host = localhost
port = 21000
username = admin
password = admin

[kubernetes]
# Kubernetes configuration (if using KubernetesExecutor)
namespace = airflow
airflow_configmap = airflow-config
worker_container_repository = apache/airflow
worker_container_tag = 2.7.0
worker_container_image_pull_policy = IfNotPresent
delete_worker_pods = True
delete_worker_pods_on_failure = False

[secrets]
# Secrets backend configuration
backend =
backend_kwargs = {}
